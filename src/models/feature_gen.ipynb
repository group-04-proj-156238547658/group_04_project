{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f2aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "def load_dataset():\n",
    "    file = open(\"./dataset/shuffle_dataset.pickle\", \"rb\")\n",
    "    dataset = pickle.load(file)\n",
    "    file.close()\n",
    "    print(\"dataset of length \" , len(dataset) , \" is loaded.....\")\n",
    "    return dataset\n",
    "\n",
    "def load_features():\n",
    "    # file = open(\"./dataset/features.pickle\", \"rb\")\n",
    "    file = open(\"./dataset/features_without_pp.pickle\", \"rb\")\n",
    "    features = pickle.load(file)\n",
    "    file.close()\n",
    "    print(\"features of length \" , len(features) , \" is loaded.....\")\n",
    "    # print(features[:100])\n",
    "    return features\n",
    "\n",
    "def find_features(w_features, t_words):\n",
    "    words = t_words\n",
    "    features = {}\n",
    "    for f in w_features:\n",
    "        features[f] = 0\n",
    "\n",
    "    for f in features:\n",
    "        if f in words:\n",
    "            features[f] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0307bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_featuresets(word_features):\n",
    "\n",
    "    documents_f = open(\"./T_T/testing.pickle\", \"rb\")\n",
    "    testing_files = pickle.load(documents_f)\n",
    "    documents_f.close()\n",
    "    print(str(len(testing_files)) + \" testing files loaded.....\")\n",
    "\n",
    "    testing_featureset = []\n",
    "\n",
    "    for index in range(len(testing_files)):\n",
    "        testing_featureset.append(\n",
    "            [find_features(word_features, testing_files[index][2]),\n",
    "             testing_files[index][1]])\n",
    "\n",
    "    save_featuresets = open(\"./feature_modeling/testing_featureset.pickle\", \"wb\")\n",
    "    pickle.dump(testing_featureset, save_featuresets)\n",
    "    save_featuresets.close()\n",
    "    print(\"testing featuresets saved.....\")\n",
    "    print(\"===============>\")\n",
    "\n",
    "    for t_index in range(10):\n",
    "        documents_f = open(\"./T_T/training\" + str(t_index + 1) +\".pickle\", \"rb\")\n",
    "        training_files = pickle.load(documents_f)\n",
    "        documents_f.close()\n",
    "        print(str(len(training_files)) + \" training files loaded.....\")\n",
    "\n",
    "        training_featureset = []\n",
    "\n",
    "        for index in range(len(training_files)):\n",
    "            training_featureset.append(\n",
    "                [find_features(word_features, training_files[index][2]),\n",
    "                 training_files[index][1]])\n",
    "\n",
    "        save_featuresets = open(\"./feature_modeling/training_featureset\" + str(t_index + 1) + \".pickle\",\"wb\")\n",
    "        pickle.dump(training_featureset, save_featuresets)\n",
    "        save_featuresets.close()\n",
    "        print(\"training featuresets \" + str(t_index + 1) + \" saved.....\")\n",
    "        print(\"===============>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b5e0571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features of length  5000  is loaded.....\n",
      "1336 testing files loaded.....\n",
      "testing featuresets saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 1 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 2 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 3 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 4 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 5 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 6 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 7 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 8 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 9 saved.....\n",
      "===============>\n",
      "1228 training files loaded.....\n",
      "training featuresets 10 saved.....\n",
      "===============>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # dataset = load_dataset()\n",
    "    features = load_features()\n",
    "    # saperate_training_and_testing_data(dataset)\n",
    "    make_featuresets(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f0657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
